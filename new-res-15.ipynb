{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:41:03.178163Z",
     "iopub.status.busy": "2024-03-24T18:41:03.177436Z",
     "iopub.status.idle": "2024-03-24T18:41:06.220327Z",
     "shell.execute_reply": "2024-03-24T18:41:06.219312Z",
     "shell.execute_reply.started": "2024-03-24T18:41:03.178120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'positive': 326 instances\n",
      "Class 'negative': 182 instances\n",
      "Class 'neutral': 34 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilroberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at distilroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model_1 (TFRobertaMo TFBaseModelOutputWit 82118400    input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 768)          0           tf_roberta_model_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            2307        global_average_pooling1d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 82,120,707\n",
      "Trainable params: 82,120,707\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, losses, Model\n",
    "import tensorflow.keras.layers as L\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Load datasets\n",
    "\n",
    "# training_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval14_res/Train/Restaurants_Train.csv\")\n",
    "# validation_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval14_res/Test/Restaurants_Test.csv\")\n",
    "\n",
    "training_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval15/Train/Restaurants_Train.csv\")\n",
    "validation_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval15/Test/Restaurants_Test.csv\")\n",
    "\n",
    "# training_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval16/Train/Restaurants_Train.csv\")\n",
    "# validation_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval16/Test/Restaurants_Test.csv\")\n",
    "\n",
    "# Drop 'conflict' polarity entries\n",
    "training_df.drop(training_df[training_df['polarity'] == 'conflict'].index, inplace=True)\n",
    "validation_df.drop(validation_df[validation_df['polarity'] == 'conflict'].index, inplace=True)\n",
    "\n",
    "# Prepare labels\n",
    "y_train = pd.get_dummies(training_df['polarity']).values\n",
    "y_valid = pd.get_dummies(validation_df['polarity']).values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "class_counts = validation_df['polarity'].value_counts()\n",
    "\n",
    "# Print class counts and their names\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"Class '{class_name}': {count} instances\")\n",
    "\n",
    "\n",
    "# # Initialization WordNetLemmatizer\n",
    "# import re\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from bs4 import BeautifulSoup\n",
    "# import emoji\n",
    "# import string\n",
    "# # Initialize the WordNetLemmatizer\n",
    "# wnl = WordNetLemmatizer()\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     \"\"\"\n",
    "#     Performs a series of preprocessing steps on the input text.\n",
    "#     Args:\n",
    "#         text (str): The input text to be preprocessed.\n",
    "#     Returns:\n",
    "#         str: The preprocessed text.\n",
    "#     \"\"\"\n",
    "#     # Remove HTML tags using BeautifulSoup.\n",
    "#     text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "#     # Remove content within square brackets.\n",
    "#     text = re.sub('\\[[^]]*\\]', '', text)\n",
    "#     # Expand contractions.\n",
    "#     #text = decontraction(text)\n",
    "#     # Remove emojis.\n",
    "#     #text = remove_emojis(text)\n",
    "#     # Remove URLs, usernames, and similar patterns.\n",
    "#     text = re.sub(r'https?://\\S+|www\\.\\S+|@[^\\s]+', '', text)\n",
    "#     # Remove punctuation using a translation table.\n",
    "#     text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "#     # Replace sequences of the same character where length > 2 with two characters.\n",
    "#     #text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "    \n",
    "#     # Remove non-alphabetical characters.\n",
    "#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "#     # Convert to lowercase to normalize the text.\n",
    "#     text = text.lower()\n",
    "#     return text\n",
    "\n",
    "# # Note: This function now includes lemmatization in the preprocessing pipeline.\n",
    "# # preprocess_text--- data cleaning with lemmatization\n",
    "# training_df['Sentence'] = training_df['Sentence'].apply(preprocess_text)\n",
    "# validation_df['Sentence'] = validation_df['Sentence'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizer and Transformer configuration\n",
    "transformer_model = 'distilroberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model)\n",
    "seq_len = 512  # Max sequence length\n",
    "batch_size = 8\n",
    "\n",
    "# Function to emphasize aspect terms in sentences\n",
    "def emphasize_aspect_terms(sentence, aspect):\n",
    "    # Example strategy: Repeat the aspect term twice and wrap with special tokens\n",
    "    emphasized_sentence = sentence.replace(aspect, f\"[ASP] {aspect} {aspect} [/ASP]\")\n",
    "    return emphasized_sentence\n",
    "\n",
    "# Apply emphasis on aspect terms\n",
    "training_df['Emphasized_Sentence'] = training_df.apply(lambda x: emphasize_aspect_terms(x['Sentence'], x['Aspect Term']), axis=1)\n",
    "validation_df['Emphasized_Sentence'] = validation_df.apply(lambda x: emphasize_aspect_terms(x['Sentence'], x['Aspect Term']), axis=1)\n",
    "\n",
    "# Tokenization\n",
    "tokenized_inputs_train = tokenizer(training_df['Emphasized_Sentence'].tolist(), max_length=seq_len, truncation=True, padding='max_length', return_tensors='tf')\n",
    "tokenized_inputs_valid = tokenizer(validation_df['Emphasized_Sentence'].tolist(), max_length=seq_len, truncation=True, padding='max_length', return_tensors='tf')\n",
    "\n",
    "# Model Building\n",
    "encoder = TFAutoModel.from_pretrained(transformer_model)\n",
    "\n",
    "input_ids = L.Input(shape=(seq_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = L.Input(shape=(seq_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "embeddings = encoder(input_ids, attention_mask=attention_mask)[0]\n",
    "pooled_output = L.GlobalAveragePooling1D()(embeddings)\n",
    "outputs = L.Dense(y_train.shape[1], activation='softmax')(pooled_output)\n",
    "\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-5), loss=losses.CategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "\n",
    "# Prepare TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs_train['input_ids'], 'attention_mask': tokenized_inputs_train['attention_mask']}, y_train))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs_valid['input_ids'], 'attention_mask': tokenized_inputs_valid['attention_mask']}, y_valid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:41:06.222557Z",
     "iopub.status.busy": "2024-03-24T18:41:06.222227Z",
     "iopub.status.idle": "2024-03-24T18:41:06.229625Z",
     "shell.execute_reply": "2024-03-24T18:41:06.228550Z",
     "shell.execute_reply.started": "2024-03-24T18:41:06.222525Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import optimizers, losses, Model\n",
    "# import tensorflow.keras.layers as L\n",
    "# from transformers import AutoTokenizer, TFAutoModel\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# # Load datasets\n",
    "\n",
    "# training_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval14_res/Train/Restaurants_Train.csv\")\n",
    "# validation_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval14_res/Test/Restaurants_Test.csv\")\n",
    "\n",
    "# # training_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval15/Train/Restaurants_Train.csv\")\n",
    "# # validation_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval15/Test/Restaurants_Test.csv\")\n",
    "\n",
    "# # training_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval16/Train/Restaurants_Train.csv\")\n",
    "# # validation_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval16/Test/Restaurants_Test.csv\")\n",
    "\n",
    "# # training_df = pd.read_csv(\"/kaggle/input/sentiment-classification/Sentihood/Train/Sentihood_train.csv\")\n",
    "# # validation_df = pd.read_csv(\"/kaggle/input/sentiment-classification/Sentihood/Test/Sentihood_test.csv\")\n",
    "\n",
    "\n",
    "# # Drop 'conflict' polarity entries\n",
    "# training_df.drop(training_df[training_df['polarity'] == 'conflict'].index, inplace=True)\n",
    "# validation_df.drop(validation_df[validation_df['polarity'] == 'conflict'].index, inplace=True)\n",
    "\n",
    "# # Prepare labels\n",
    "# y_train = pd.get_dummies(training_df['polarity']).values\n",
    "# y_valid = pd.get_dummies(validation_df['polarity']).values\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Assuming your DataFrame is named df\n",
    "# class_counts = training_df['polarity'].value_counts()\n",
    "\n",
    "# # Print class counts and their names\n",
    "# for class_name, count in class_counts.items():\n",
    "#     print(f\"Class '{class_name}': {count} instances\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:41:06.231463Z",
     "iopub.status.busy": "2024-03-24T18:41:06.230993Z",
     "iopub.status.idle": "2024-03-24T18:41:06.241823Z",
     "shell.execute_reply": "2024-03-24T18:41:06.240915Z",
     "shell.execute_reply.started": "2024-03-24T18:41:06.231420Z"
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# # Define a checkpoint callback\n",
    "# checkpoint_path = \"/kaggle/working/best_model_tr.h5\"\n",
    "# checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# # Training\n",
    "# model.fit(train_dataset.shuffle(10000).batch(batch_size), \n",
    "#           validation_data=valid_dataset.batch(batch_size), \n",
    "#           epochs=10,\n",
    "#           verbose=1,\n",
    "#           callbacks=[checkpoint])  # Add checkpoint callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:41:06.245356Z",
     "iopub.status.busy": "2024-03-24T18:41:06.244923Z",
     "iopub.status.idle": "2024-03-24T18:41:06.253841Z",
     "shell.execute_reply": "2024-03-24T18:41:06.252921Z",
     "shell.execute_reply.started": "2024-03-24T18:41:06.245312Z"
    }
   },
   "outputs": [],
   "source": [
    "# from transformers import TFRobertaModel  # Import the necessary layer\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the model with custom objects argument\n",
    "# custom_objects = {'TFRobertaModel': TFRobertaModel}\n",
    "# model = load_model(\"/kaggle/working/best_model_tr.h5\", custom_objects=custom_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:41:06.255480Z",
     "iopub.status.busy": "2024-03-24T18:41:06.255097Z",
     "iopub.status.idle": "2024-03-24T18:41:17.395238Z",
     "shell.execute_reply": "2024-03-24T18:41:17.394127Z",
     "shell.execute_reply.started": "2024-03-24T18:41:06.255424Z"
    }
   },
   "outputs": [],
   "source": [
    "# Making predictions on the validation dataset\n",
    "y_pred = model.predict(valid_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:41:17.396951Z",
     "iopub.status.busy": "2024-03-24T18:41:17.396629Z",
     "iopub.status.idle": "2024-03-24T18:41:17.402375Z",
     "shell.execute_reply": "2024-03-24T18:41:17.401335Z",
     "shell.execute_reply.started": "2024-03-24T18:41:17.396922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(542, 3)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:41:17.404021Z",
     "iopub.status.busy": "2024-03-24T18:41:17.403679Z",
     "iopub.status.idle": "2024-03-24T18:41:17.419991Z",
     "shell.execute_reply": "2024-03-24T18:41:17.418817Z",
     "shell.execute_reply.started": "2024-03-24T18:41:17.403992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.16882428492793214\n",
      "Precision: 0.11275717923230894\n",
      "Recall: 0.33579335793357934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# Converting probabilià§®ties to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_valid, axis=1)\n",
    "\n",
    "\n",
    "# Computing F1 score\n",
    "f1 = f1_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Computing precision\n",
    "precision = precision_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Computing recall\n",
    "recall = recall_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:41:17.421890Z",
     "iopub.status.busy": "2024-03-24T18:41:17.421456Z",
     "iopub.status.idle": "2024-03-24T18:41:17.426812Z",
     "shell.execute_reply": "2024-03-24T18:41:17.425719Z",
     "shell.execute_reply.started": "2024-03-24T18:41:17.421845Z"
    }
   },
   "outputs": [],
   "source": [
    "# F1 Score: 0.7940415317576729\n",
    "# Precision: 0.799109346633389\n",
    "# Recall: 0.7978056426332288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:41:17.428637Z",
     "iopub.status.busy": "2024-03-24T18:41:17.428301Z",
     "iopub.status.idle": "2024-03-24T18:41:17.448994Z",
     "shell.execute_reply": "2024-03-24T18:41:17.448003Z",
     "shell.execute_reply.started": "2024-03-24T18:41:17.428609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model_1 (TFRobertaMo TFBaseModelOutputWit 82118400    input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 768)          0           tf_roberta_model_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 82,118,400\n",
      "Trainable params: 82,118,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Access the layers of the model\n",
    "layers = model.layers\n",
    "output_tensor = layers[-2].output\n",
    "new_model = Model(inputs=model.input, outputs=output_tensor)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:41:17.453191Z",
     "iopub.status.busy": "2024-03-24T18:41:17.452334Z",
     "iopub.status.idle": "2024-03-24T18:41:17.456951Z",
     "shell.execute_reply": "2024-03-24T18:41:17.456016Z",
     "shell.execute_reply.started": "2024-03-24T18:41:17.453159Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Making predictions on the validation dataset\n",
    "# y_pred = new_model.predict(valid_dataset.batch(batch_size))\n",
    "# print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:41:17.459440Z",
     "iopub.status.busy": "2024-03-24T18:41:17.459147Z",
     "iopub.status.idle": "2024-03-24T18:41:51.138452Z",
     "shell.execute_reply": "2024-03-24T18:41:51.137382Z",
     "shell.execute_reply.started": "2024-03-24T18:41:17.459413Z"
    }
   },
   "outputs": [],
   "source": [
    "new_traning_data=new_model.predict(train_dataset.batch(batch_size))\n",
    "new_valid_dataset=new_model.predict(valid_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:41:51.140324Z",
     "iopub.status.busy": "2024-03-24T18:41:51.139954Z",
     "iopub.status.idle": "2024-03-24T18:41:51.147018Z",
     "shell.execute_reply": "2024-03-24T18:41:51.145567Z",
     "shell.execute_reply.started": "2024-03-24T18:41:51.140292Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# # Assuming new_traning_data and new_valid_dataset are features extracted by your neural network\n",
    "# # and train_labels, valid_labels are the corresponding labels\n",
    "# svm = SVC(kernel='rbf')  # You can choose different kernels like 'rbf', 'poly', etc.\n",
    "\n",
    "# # Reshape the data if necessary\n",
    "# new_traning_data = new_traning_data.reshape(new_traning_data.shape[0], -1)\n",
    "# new_valid_dataset = new_valid_dataset.reshape(new_valid_dataset.shape[0], -1)\n",
    "# y_train_single = np.argmax(y_train, axis=1)\n",
    "\n",
    "# # Train the SVM\n",
    "# svm.fit(new_traning_data, y_train_single)\n",
    "\n",
    "# # Evaluate the SVM on the validation dataset\n",
    "# svm_predictions = svm.predict(new_valid_dataset)\n",
    "# y_valid_single = np.argmax(y_valid, axis=1)\n",
    "# # Calculate F1 score\n",
    "# f1 = f1_score(y_valid_single, svm_predictions, average='micro')\n",
    "\n",
    "# print(\"SVM F1 Score on validation dataset:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:41:51.149407Z",
     "iopub.status.busy": "2024-03-24T18:41:51.148709Z",
     "iopub.status.idle": "2024-03-24T18:41:51.157125Z",
     "shell.execute_reply": "2024-03-24T18:41:51.156374Z",
     "shell.execute_reply.started": "2024-03-24T18:41:51.149360Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_valid_single, svm_predictions)\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={\"size\": 18}) # Set annotation font size\n",
    "# plt.xlabel('Predicted labels', fontsize=18) # Set x-axis label font size\n",
    "# plt.ylabel('True labels', fontsize=18) # Set y-axis label font size\n",
    "# plt.title('Confusion Matrix', fontsize=20) # Set title font size\n",
    "# plt.xticks(fontsize=18) # Set x-axis tick font size\n",
    "# plt.yticks(fontsize=18) # Set y-axis tick font size\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:41:51.158805Z",
     "iopub.status.busy": "2024-03-24T18:41:51.158415Z",
     "iopub.status.idle": "2024-03-24T18:42:02.517244Z",
     "shell.execute_reply": "2024-03-24T18:42:02.516163Z",
     "shell.execute_reply.started": "2024-03-24T18:41:51.158768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: stellargraph in /opt/conda/lib/python3.7/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (1.19.5)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (3.5.2)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (4.0.1)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (2.5)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (1.0.2)\n",
      "Requirement already satisfied: tensorflow>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (2.6.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (1.7.3)\n",
      "Requirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (1.3.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim>=3.4.0->stellargraph) (5.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->stellargraph) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->stellargraph) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->stellargraph) (9.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->stellargraph) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->stellargraph) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->stellargraph) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->stellargraph) (3.0.9)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.2->stellargraph) (5.1.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24->stellargraph) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20->stellargraph) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20->stellargraph) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions<3.11,>=3.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (3.10.0.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (3.20.1)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (1.1.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (1.6.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (0.37.1)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (1.12)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (2.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (1.43.0)\n",
      "Requirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (5.0)\n",
      "Requirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (2.6.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (1.15.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (0.15.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (3.3.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (0.4.0)\n",
      "Requirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (2.6.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (0.2.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (1.12.1)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=2.1.0->stellargraph) (1.5.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (2.1.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (59.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (3.3.7)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (1.35.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (0.2.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (4.11.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (2.0.12)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install stellargraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:42:02.519869Z",
     "iopub.status.busy": "2024-03-24T18:42:02.519067Z",
     "iopub.status.idle": "2024-03-24T18:42:02.527450Z",
     "shell.execute_reply": "2024-03-24T18:42:02.526367Z",
     "shell.execute_reply.started": "2024-03-24T18:42:02.519824Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = np.concatenate((new_traning_data, new_valid_dataset), axis=0), np.concatenate((y_train, y_valid), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:42:02.529388Z",
     "iopub.status.busy": "2024-03-24T18:42:02.529004Z",
     "iopub.status.idle": "2024-03-24T18:42:02.535869Z",
     "shell.execute_reply": "2024-03-24T18:42:02.534838Z",
     "shell.execute_reply.started": "2024-03-24T18:42:02.529350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204, 768)\n",
      "(542, 768)\n"
     ]
    }
   ],
   "source": [
    "print(new_traning_data.shape)\n",
    "print(new_valid_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:47:41.856500Z",
     "iopub.status.busy": "2024-03-24T18:47:41.855571Z",
     "iopub.status.idle": "2024-03-24T18:47:42.023315Z",
     "shell.execute_reply": "2024-03-24T18:47:42.022308Z",
     "shell.execute_reply.started": "2024-03-24T18:47:41.856451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 10, 768)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           [(None, 50, 768)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 1, 768)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 1, 10, 768)   0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 10, 5, 768)   0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 1, 768)       0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 1, 10, 768)   0           reshape_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 10, 768)      0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 10, 5, 768)   0           reshape_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mean_aggregator_6 (MeanAggregat multiple             49216       dropout_57[0][0]                 \n",
      "                                                                 dropout_56[0][0]                 \n",
      "                                                                 dropout_59[0][0]                 \n",
      "                                                                 dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 1, 10, 64)    0           mean_aggregator_6[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 1, 64)        0           mean_aggregator_6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 1, 10, 64)    0           reshape_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mean_aggregator_7 (MeanAggregat (None, 1, 32)        2080        dropout_61[0][0]                 \n",
      "                                                                 dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 32)           0           mean_aggregator_7[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 32)           0           reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 3)            99          lambda_3[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 51,395\n",
      "Trainable params: 51,395\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import stellargraph as sg\n",
    "from stellargraph.layer import GraphSAGE\n",
    "from stellargraph.mapper import GraphSAGENodeGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "\n",
    "# Construct a graph from the DataFrame\n",
    "graph = sg.StellarGraph(nodes=df)\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.index, y, test_size=0.2160, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Define the GraphSAGE model\n",
    "generator = GraphSAGENodeGenerator(graph, batch_size=8, num_samples=[10, 5])\n",
    "train_gen = generator.flow(X_train, y_train)\n",
    "\n",
    "graphsage_model = GraphSAGE(\n",
    "    layer_sizes=[64, 32],\n",
    "    generator=generator,\n",
    "    bias=True,\n",
    "    dropout=0.2,\n",
    "    normalize=\"l2\"\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "x_inp, x_out = graphsage_model.in_out_tensors()\n",
    "prediction = layers.Dense(units=3, activation=\"softmax\")(x_out)\n",
    "\n",
    "model = models.Model(inputs=x_inp, outputs=prediction)\n",
    "learning_rate = 0.001  # Set your desired learning rate here\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Define a checkpoint callback\n",
    "checkpoint_path = \"/kaggle/working/best_model_gnn.h5\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T19:00:51.263411Z",
     "iopub.status.busy": "2024-03-24T19:00:51.262630Z",
     "iopub.status.idle": "2024-03-24T19:00:57.011317Z",
     "shell.execute_reply": "2024-03-24T19:00:57.010244Z",
     "shell.execute_reply.started": "2024-03-24T19:00:51.263379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "171/171 [==============================] - 1s 6ms/step - loss: 0.2271 - acc: 0.9079 - val_loss: 0.8719 - val_acc: 0.7328\n",
      "Epoch 2/5\n",
      "171/171 [==============================] - 1s 6ms/step - loss: 0.2158 - acc: 0.9167 - val_loss: 0.8357 - val_acc: 0.7725\n",
      "Epoch 3/5\n",
      "171/171 [==============================] - 1s 6ms/step - loss: 0.2303 - acc: 0.9057 - val_loss: 0.8340 - val_acc: 0.7646\n",
      "Epoch 4/5\n",
      "171/171 [==============================] - 1s 6ms/step - loss: 0.2397 - acc: 0.9094 - val_loss: 0.8359 - val_acc: 0.7646\n",
      "Epoch 5/5\n",
      "171/171 [==============================] - 1s 6ms/step - loss: 0.2263 - acc: 0.9123 - val_loss: 0.8193 - val_acc: 0.7937\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.8193 - acc: 0.7937\n",
      "Test accuracy: 0.7936508059501648\n",
      "F1 Score: 0.7686060020716105\n",
      "Precision: 0.807033149498903\n",
      "Recall: 0.7936507936507936\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the generator\n",
    "history = model.fit(train_gen,\n",
    "                    epochs=5,\n",
    "                    validation_data=generator.flow(X_test, y_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpoint])  # Add checkpoint callback\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(generator.flow(X_test, y_test))\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Making predictions on the validation dataset\n",
    "y_pred = model.predict(generator.flow(X_test, y_test))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# Converting probabilities to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "\n",
    "# Computing F1 score\n",
    "f1 = f1_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Computing precision\n",
    "precision = precision_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Computing recall\n",
    "recall = recall_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T18:59:27.481732Z",
     "iopub.status.busy": "2024-03-24T18:59:27.481329Z",
     "iopub.status.idle": "2024-03-24T18:59:27.703301Z",
     "shell.execute_reply": "2024-03-24T18:59:27.702415Z",
     "shell.execute_reply.started": "2024-03-24T18:59:27.481698Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T19:01:29.415754Z",
     "iopub.status.busy": "2024-03-24T19:01:29.415375Z",
     "iopub.status.idle": "2024-03-24T19:01:29.429044Z",
     "shell.execute_reply": "2024-03-24T19:01:29.427784Z",
     "shell.execute_reply.started": "2024-03-24T19:01:29.415723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7686060020716105\n",
      "Precision: 0.807033149498903\n",
      "Recall: 0.7936507936507936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# Converting probabilities to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "\n",
    "# Computing F1 score\n",
    "f1 = f1_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Computing precision\n",
    "precision = precision_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Computing recall\n",
    "recall = recall_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "print(\"Recall:\", recall)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4451489,
     "sourceId": 7641359,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30203,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
