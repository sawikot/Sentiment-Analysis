{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:38:43.142438Z",
     "iopub.status.busy": "2024-03-24T15:38:43.142010Z",
     "iopub.status.idle": "2024-03-24T15:39:38.899892Z",
     "shell.execute_reply": "2024-03-24T15:39:38.898809Z",
     "shell.execute_reply.started": "2024-03-24T15:38:43.142346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'positive': 728 instances\n",
      "Class 'neutral': 196 instances\n",
      "Class 'negative': 196 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/bs4/__init__.py:439: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed431aa5c904535b4da3a8b5d8869a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8521a0e8e224e3b8c44ca31c124ee82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a4b32cba324019b5ce20ec618f8606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdeddc59c58349f58dccfc12e39f6919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c522ec76c294692adb1f94c35cf232f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a7ab23d9304984897c6a34b3784b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/465M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilroberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at distilroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode TFBaseModelOutputWit 82118400    input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            2307        global_average_pooling1d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 82,120,707\n",
      "Trainable params: 82,120,707\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, losses, Model\n",
    "import tensorflow.keras.layers as L\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "training_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval14_res/Train/Restaurants_Train.csv\")\n",
    "validation_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval14_res/Test/Restaurants_Test.csv\")\n",
    "\n",
    "\n",
    "# Drop 'conflict' polarity entries\n",
    "training_df.drop(training_df[training_df['polarity'] == 'conflict'].index, inplace=True)\n",
    "validation_df.drop(validation_df[validation_df['polarity'] == 'conflict'].index, inplace=True)\n",
    "\n",
    "# Prepare labels\n",
    "y_train = pd.get_dummies(training_df['polarity']).values\n",
    "y_valid = pd.get_dummies(validation_df['polarity']).values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "class_counts = validation_df['polarity'].value_counts()\n",
    "\n",
    "# Print class counts and their names\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"Class '{class_name}': {count} instances\")\n",
    "\n",
    "\n",
    "# Initialization WordNetLemmatizer\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji\n",
    "import string\n",
    "# Initialize the WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Performs a series of preprocessing steps on the input text.\n",
    "    Args:\n",
    "        text (str): The input text to be preprocessed.\n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    \"\"\"\n",
    "    # Remove HTML tags using BeautifulSoup.\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Remove content within square brackets.\n",
    "    text = re.sub('\\[[^]]*\\]', '', text)\n",
    "    # Expand contractions.\n",
    "    #text = decontraction(text)\n",
    "    # Remove emojis.\n",
    "    #text = remove_emojis(text)\n",
    "    # Remove URLs, usernames, and similar patterns.\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+|@[^\\s]+', '', text)\n",
    "    # Remove punctuation using a translation table.\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Replace sequences of the same character where length > 2 with two characters.\n",
    "    #text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "    \n",
    "    # Remove non-alphabetical characters.\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    # Convert to lowercase to normalize the text.\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Note: This function now includes lemmatization in the preprocessing pipeline.\n",
    "# preprocess_text--- data cleaning with lemmatization\n",
    "training_df['Sentence'] = training_df['Sentence'].apply(preprocess_text)\n",
    "validation_df['Sentence'] = validation_df['Sentence'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizer and Transformer configuration\n",
    "transformer_model = 'distilroberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model)\n",
    "seq_len = 512  # Max sequence length\n",
    "batch_size = 8\n",
    "\n",
    "# Function to emphasize aspect terms in sentences\n",
    "def emphasize_aspect_terms(sentence, aspect):\n",
    "    # Example strategy: Repeat the aspect term twice and wrap with special tokens\n",
    "    emphasized_sentence = sentence.replace(aspect, f\"[ASP] {aspect} {aspect} [/ASP]\")\n",
    "    return emphasized_sentence\n",
    "\n",
    "# Apply emphasis on aspect terms\n",
    "training_df['Emphasized_Sentence'] = training_df.apply(lambda x: emphasize_aspect_terms(x['Sentence'], x['Aspect Term']), axis=1)\n",
    "validation_df['Emphasized_Sentence'] = validation_df.apply(lambda x: emphasize_aspect_terms(x['Sentence'], x['Aspect Term']), axis=1)\n",
    "\n",
    "# Tokenization\n",
    "tokenized_inputs_train = tokenizer(training_df['Emphasized_Sentence'].tolist(), max_length=seq_len, truncation=True, padding='max_length', return_tensors='tf')\n",
    "tokenized_inputs_valid = tokenizer(validation_df['Emphasized_Sentence'].tolist(), max_length=seq_len, truncation=True, padding='max_length', return_tensors='tf')\n",
    "\n",
    "# Model Building\n",
    "encoder = TFAutoModel.from_pretrained(transformer_model)\n",
    "\n",
    "input_ids = L.Input(shape=(seq_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = L.Input(shape=(seq_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "embeddings = encoder(input_ids, attention_mask=attention_mask)[0]\n",
    "pooled_output = L.GlobalAveragePooling1D()(embeddings)\n",
    "outputs = L.Dense(y_train.shape[1], activation='softmax')(pooled_output)\n",
    "\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-5), loss=losses.CategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "\n",
    "# Prepare TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs_train['input_ids'], 'attention_mask': tokenized_inputs_train['attention_mask']}, y_train))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs_valid['input_ids'], 'attention_mask': tokenized_inputs_valid['attention_mask']}, y_valid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:39:38.903455Z",
     "iopub.status.busy": "2024-03-24T15:39:38.902205Z",
     "iopub.status.idle": "2024-03-24T15:39:38.940216Z",
     "shell.execute_reply": "2024-03-24T15:39:38.939269Z",
     "shell.execute_reply.started": "2024-03-24T15:39:38.903407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'positive': 2164 instances\n",
      "Class 'negative': 805 instances\n",
      "Class 'neutral': 633 instances\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, losses, Model\n",
    "import tensorflow.keras.layers as L\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Load datasets\n",
    "\n",
    "training_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval14_res/Train/Restaurants_Train.csv\")\n",
    "validation_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval14_res/Test/Restaurants_Test.csv\")\n",
    "\n",
    "# training_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval15/Train/Restaurants_Train.csv\")\n",
    "# validation_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval15/Test/Restaurants_Test.csv\")\n",
    "\n",
    "# training_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval16/Train/Restaurants_Train.csv\")\n",
    "# validation_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval16/Test/Restaurants_Test.csv\")\n",
    "\n",
    "# training_df = pd.read_csv(\"/kaggle/input/sentiment-classification/Sentihood/Train/Sentihood_train.csv\")\n",
    "# validation_df = pd.read_csv(\"/kaggle/input/sentiment-classification/Sentihood/Test/Sentihood_test.csv\")\n",
    "\n",
    "\n",
    "# Drop 'conflict' polarity entries\n",
    "training_df.drop(training_df[training_df['polarity'] == 'conflict'].index, inplace=True)\n",
    "validation_df.drop(validation_df[validation_df['polarity'] == 'conflict'].index, inplace=True)\n",
    "\n",
    "# Prepare labels\n",
    "y_train = pd.get_dummies(training_df['polarity']).values\n",
    "y_valid = pd.get_dummies(validation_df['polarity']).values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "class_counts = training_df['polarity'].value_counts()\n",
    "\n",
    "# Print class counts and their names\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"Class '{class_name}': {count} instances\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:39:38.941873Z",
     "iopub.status.busy": "2024-03-24T15:39:38.941466Z",
     "iopub.status.idle": "2024-03-24T15:39:38.948630Z",
     "shell.execute_reply": "2024-03-24T15:39:38.947643Z",
     "shell.execute_reply.started": "2024-03-24T15:39:38.941844Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from transformers import pipeline, AutoTokenizer, TFAutoModel\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# # Load datasets\n",
    "# training_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval14_res/Train/Restaurants_Train.csv\")\n",
    "# validation_df = pd.read_csv(\"/kaggle/input/sentiment-classification/SemEval14_res/Test/Restaurants_Test.csv\")\n",
    "\n",
    "# # Drop 'conflict' polarity entries\n",
    "# training_df.drop(training_df[training_df['polarity'] == 'conflict'].index, inplace=True)\n",
    "# validation_df.drop(validation_df[validation_df['polarity'] == 'conflict'].index, inplace=True)\n",
    "\n",
    "# # Initialize the BART model and tokenizer for paraphrasing\n",
    "# paraphrase_pipeline = pipeline(\"text2text-generation\", model=\"facebook/bart-large-cnn\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# # Example function to paraphrase sentences\n",
    "# def paraphrase_sentence(sentence):\n",
    "#     paraphrases = paraphrase_pipeline(sentence, max_length=50, num_return_sequences=1)  # Change num_return_sequences here\n",
    "#     return [paraphrase['generated_text'] for paraphrase in paraphrases]\n",
    "\n",
    "# # Function to augment data\n",
    "# def augment_data(sentences):\n",
    "#     augmented_sentences = []\n",
    "#     for sentence in sentences:\n",
    "#         paraphrases = paraphrase_sentence(sentence)\n",
    "#         augmented_sentences.extend(paraphrases)\n",
    "#     return augmented_sentences\n",
    "\n",
    "# # Augmenting training data\n",
    "# augmented_sentences = augment_data(training_df['Sentence'].tolist())\n",
    "\n",
    "# # Concatenate original and augmented sentences\n",
    "# augmented_training_df = pd.DataFrame({'Sentence': training_df['Sentence'].tolist() + augmented_sentences, \n",
    "#                                       'polarity': training_df['polarity'].tolist() * (len(augmented_sentences) + 1)})\n",
    "\n",
    "# # Tokenization\n",
    "# seq_len = 512  # Max sequence length\n",
    "# tokenized_inputs_train_augmented = tokenizer(augmented_training_df['Sentence'].tolist(), max_length=seq_len, truncation=True, padding='max_length', return_tensors='tf')\n",
    "# tokenized_inputs_valid = tokenizer(validation_df['Sentence'].tolist(), max_length=seq_len, truncation=True, padding='max_length', return_tensors='tf')\n",
    "\n",
    "# # Prepare labels\n",
    "# y_train_augmented = pd.get_dummies(augmented_training_df['polarity']).values\n",
    "# y_valid = pd.get_dummies(validation_df['polarity']).values\n",
    "\n",
    "# # Class balancing using SMOTE\n",
    "# smote = SMOTE()\n",
    "# X_train_reshaped = tokenized_inputs_train_augmented['input_ids'].numpy().reshape(-1, seq_len)\n",
    "# y_train_reshaped = np.argmax(y_train_augmented, axis=1)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_reshaped, y_train_reshaped)\n",
    "# X_train_resampled = X_train_resampled.reshape(-1, seq_len)\n",
    "# y_train_resampled = tf.keras.utils.to_categorical(y_train_resampled, num_classes=y_train_augmented.shape[1])\n",
    "\n",
    "# # Convert back to TensorFlow Dataset\n",
    "# train_dataset_resampled = tf.data.Dataset.from_tensor_slices(({'input_ids': X_train_resampled, 'attention_mask': tokenized_inputs_train_augmented['attention_mask']}, y_train_resampled))\n",
    "# valid_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs_valid['input_ids'], 'attention_mask': tokenized_inputs_valid['attention_mask']}, y_valid))\n",
    "\n",
    "# # Shuffle and batch the resampled dataset\n",
    "# train_dataset_resampled = train_dataset_resampled.shuffle(len(X_train_resampled)).batch(batch_size)\n",
    "# valid_dataset = valid_dataset.batch(batch_size)\n",
    "\n",
    "# # Now you can use train_dataset_resampled for training and valid_dataset for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:39:38.952129Z",
     "iopub.status.busy": "2024-03-24T15:39:38.951296Z",
     "iopub.status.idle": "2024-03-24T15:39:38.967285Z",
     "shell.execute_reply": "2024-03-24T15:39:38.966443Z",
     "shell.execute_reply.started": "2024-03-24T15:39:38.952093Z"
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# # Define a checkpoint callback\n",
    "# checkpoint_path = \"/kaggle/working/best_model_tr.h5\"\n",
    "# checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# # Training\n",
    "# model.fit(train_dataset.shuffle(10000).batch(batch_size), \n",
    "#           validation_data=valid_dataset.batch(batch_size), \n",
    "#           epochs=10,\n",
    "#           verbose=1,\n",
    "#           callbacks=[checkpoint])  # Add checkpoint callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:39:38.968677Z",
     "iopub.status.busy": "2024-03-24T15:39:38.968368Z",
     "iopub.status.idle": "2024-03-24T15:39:38.979213Z",
     "shell.execute_reply": "2024-03-24T15:39:38.978449Z",
     "shell.execute_reply.started": "2024-03-24T15:39:38.968649Z"
    }
   },
   "outputs": [],
   "source": [
    "# from transformers import TFRobertaModel  # Import the necessary layer\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the model with custom objects argument\n",
    "# custom_objects = {'TFRobertaModel': TFRobertaModel}\n",
    "# model = load_model(\"/kaggle/working/best_model_tr.h5\", custom_objects=custom_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:39:38.980453Z",
     "iopub.status.busy": "2024-03-24T15:39:38.980176Z",
     "iopub.status.idle": "2024-03-24T15:40:00.857691Z",
     "shell.execute_reply": "2024-03-24T15:40:00.856609Z",
     "shell.execute_reply.started": "2024-03-24T15:39:38.980426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Making predictions on the validation dataset\n",
    "y_pred = model.predict(valid_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:40:00.859571Z",
     "iopub.status.busy": "2024-03-24T15:40:00.859166Z",
     "iopub.status.idle": "2024-03-24T15:40:00.865623Z",
     "shell.execute_reply": "2024-03-24T15:40:00.864638Z",
     "shell.execute_reply.started": "2024-03-24T15:40:00.859534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1120, 3)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:40:00.867292Z",
     "iopub.status.busy": "2024-03-24T15:40:00.866927Z",
     "iopub.status.idle": "2024-03-24T15:40:00.892741Z",
     "shell.execute_reply": "2024-03-24T15:40:00.891642Z",
     "shell.execute_reply.started": "2024-03-24T15:40:00.867257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.05212765957446808\n",
      "Precision: 0.030624999999999996\n",
      "Recall: 0.175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# Converting probabilities to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_valid, axis=1)\n",
    "\n",
    "\n",
    "# Computing F1 score\n",
    "f1 = f1_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Computing precision\n",
    "precision = precision_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Computing recall\n",
    "recall = recall_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:40:00.894735Z",
     "iopub.status.busy": "2024-03-24T15:40:00.894405Z",
     "iopub.status.idle": "2024-03-24T15:40:00.900487Z",
     "shell.execute_reply": "2024-03-24T15:40:00.899310Z",
     "shell.execute_reply.started": "2024-03-24T15:40:00.894705Z"
    }
   },
   "outputs": [],
   "source": [
    "# F1 Score: 0.7940415317576729\n",
    "# Precision: 0.799109346633389\n",
    "# Recall: 0.7978056426332288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:40:00.905300Z",
     "iopub.status.busy": "2024-03-24T15:40:00.904901Z",
     "iopub.status.idle": "2024-03-24T15:40:00.931332Z",
     "shell.execute_reply": "2024-03-24T15:40:00.930164Z",
     "shell.execute_reply.started": "2024-03-24T15:40:00.905265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode TFBaseModelOutputWit 82118400    input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_roberta_model[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 82,118,400\n",
      "Trainable params: 82,118,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Access the layers of the model\n",
    "layers = model.layers\n",
    "output_tensor = layers[-2].output\n",
    "new_model = Model(inputs=model.input, outputs=output_tensor)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:40:00.932914Z",
     "iopub.status.busy": "2024-03-24T15:40:00.932556Z",
     "iopub.status.idle": "2024-03-24T15:40:00.937715Z",
     "shell.execute_reply": "2024-03-24T15:40:00.936523Z",
     "shell.execute_reply.started": "2024-03-24T15:40:00.932884Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Making predictions on the validation dataset\n",
    "# y_pred = new_model.predict(valid_dataset.batch(batch_size))\n",
    "# print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:40:00.939461Z",
     "iopub.status.busy": "2024-03-24T15:40:00.939135Z",
     "iopub.status.idle": "2024-03-24T15:41:31.709491Z",
     "shell.execute_reply": "2024-03-24T15:41:31.708363Z",
     "shell.execute_reply.started": "2024-03-24T15:40:00.939427Z"
    }
   },
   "outputs": [],
   "source": [
    "new_traning_data=new_model.predict(train_dataset.batch(batch_size))\n",
    "new_valid_dataset=new_model.predict(valid_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:41:31.711296Z",
     "iopub.status.busy": "2024-03-24T15:41:31.710979Z",
     "iopub.status.idle": "2024-03-24T15:41:31.717366Z",
     "shell.execute_reply": "2024-03-24T15:41:31.716415Z",
     "shell.execute_reply.started": "2024-03-24T15:41:31.711268Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# # Assuming new_traning_data and new_valid_dataset are features extracted by your neural network\n",
    "# # and train_labels, valid_labels are the corresponding labels\n",
    "# svm = SVC(kernel='rbf')  # You can choose different kernels like 'rbf', 'poly', etc.\n",
    "\n",
    "# # Reshape the data if necessary\n",
    "# new_traning_data = new_traning_data.reshape(new_traning_data.shape[0], -1)\n",
    "# new_valid_dataset = new_valid_dataset.reshape(new_valid_dataset.shape[0], -1)\n",
    "# y_train_single = np.argmax(y_train, axis=1)\n",
    "\n",
    "# # Train the SVM\n",
    "# svm.fit(new_traning_data, y_train_single)\n",
    "\n",
    "# # Evaluate the SVM on the validation dataset\n",
    "# svm_predictions = svm.predict(new_valid_dataset)\n",
    "# y_valid_single = np.argmax(y_valid, axis=1)\n",
    "# # Calculate F1 score\n",
    "# f1 = f1_score(y_valid_single, svm_predictions, average='micro')\n",
    "\n",
    "# print(\"SVM F1 Score on validation dataset:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:41:31.719122Z",
     "iopub.status.busy": "2024-03-24T15:41:31.718743Z",
     "iopub.status.idle": "2024-03-24T15:41:31.738160Z",
     "shell.execute_reply": "2024-03-24T15:41:31.737217Z",
     "shell.execute_reply.started": "2024-03-24T15:41:31.719080Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_valid_single, svm_predictions)\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={\"size\": 18}) # Set annotation font size\n",
    "# plt.xlabel('Predicted labels', fontsize=18) # Set x-axis label font size\n",
    "# plt.ylabel('True labels', fontsize=18) # Set y-axis label font size\n",
    "# plt.title('Confusion Matrix', fontsize=20) # Set title font size\n",
    "# plt.xticks(fontsize=18) # Set x-axis tick font size\n",
    "# plt.yticks(fontsize=18) # Set y-axis tick font size\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:41:31.739653Z",
     "iopub.status.busy": "2024-03-24T15:41:31.739343Z",
     "iopub.status.idle": "2024-03-24T15:42:26.580213Z",
     "shell.execute_reply": "2024-03-24T15:42:26.578627Z",
     "shell.execute_reply.started": "2024-03-24T15:41:31.739626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting stellargraph\n",
      "  Downloading stellargraph-1.2.1-py3-none-any.whl (435 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.2/435.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (2.6.4)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (4.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (1.21.6)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (2.5)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (3.5.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (1.7.3)\n",
      "Requirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.7/site-packages (from stellargraph) (1.3.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim>=3.4.0->stellargraph) (5.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->stellargraph) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->stellargraph) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->stellargraph) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->stellargraph) (9.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->stellargraph) (4.33.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->stellargraph) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->stellargraph) (3.0.9)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.2->stellargraph) (5.1.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24->stellargraph) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20->stellargraph) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20->stellargraph) (1.1.0)\n",
      "Collecting typing-extensions<3.11,>=3.7\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (2.6.0)\n",
      "Requirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (5.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (1.43.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (3.1.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (0.2.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (0.37.1)\n",
      "Collecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting numpy>=1.14\n",
      "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (1.12)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (0.4.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (2.6.0)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (2.6.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (3.20.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (1.1.2)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->stellargraph) (1.1.0)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=2.1.0->stellargraph) (1.5.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (0.4.6)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (1.35.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (2.1.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (59.8.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (0.6.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (4.11.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (2022.6.15)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->stellargraph) (3.2.0)\n",
      "Building wheels for collected packages: wrapt\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=77062 sha256=911af9f6162dcec344302337d6c91cc29f60c4313efdf808b2c37b100d10cf79\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "Successfully built wrapt\n",
      "Installing collected packages: wrapt, typing-extensions, six, numpy, absl-py, stellargraph\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.14.1\n",
      "    Uninstalling wrapt-1.14.1:\n",
      "      Successfully uninstalled wrapt-1.14.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.1.1\n",
      "    Uninstalling typing_extensions-4.1.1:\n",
      "      Successfully uninstalled typing_extensions-4.1.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.6\n",
      "    Uninstalling numpy-1.21.6:\n",
      "      Successfully uninstalled numpy-1.21.6\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 1.0.0\n",
      "    Uninstalling absl-py-1.0.0:\n",
      "      Successfully uninstalled absl-py-1.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\n",
      "dask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\n",
      "beatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\n",
      "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
      "tfx-bsl 1.8.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\n",
      "tensorflow-transform 1.8.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<2.9,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\n",
      "tensorflow-serving-api 2.8.0 requires tensorflow<3,>=2.8.0, but you have tensorflow 2.6.4 which is incompatible.\n",
      "rich 12.4.4 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "pytorch-lightning 1.6.4 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "pytools 2022.1.9 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "pdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.2 which is incompatible.\n",
      "grpcio-status 1.46.3 requires grpcio>=1.46.3, but you have grpcio 1.43.0 which is incompatible.\n",
      "google-cloud-aiplatform 0.6.0a1 requires google-cloud-storage<2.0.0dev,>=1.26.0, but you have google-cloud-storage 2.1.0 which is incompatible.\n",
      "flax 0.5.1 requires rich~=11.1.0, but you have rich 12.4.4 which is incompatible.\n",
      "flax 0.5.1 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "flake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.11.4 which is incompatible.\n",
      "featuretools 1.9.2 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\n",
      "dask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\n",
      "dask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\n",
      "apache-beam 2.39.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\n",
      "apache-beam 2.39.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.4 which is incompatible.\n",
      "aioitertools 0.10.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "aiobotocore 2.3.3 requires botocore<1.24.22,>=1.24.21, but you have botocore 1.27.10 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-0.15.0 numpy-1.19.5 six-1.15.0 stellargraph-1.2.1 typing-extensions-3.10.0.2 wrapt-1.12.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install stellargraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:42:26.582798Z",
     "iopub.status.busy": "2024-03-24T15:42:26.582303Z",
     "iopub.status.idle": "2024-03-24T15:42:26.591695Z",
     "shell.execute_reply": "2024-03-24T15:42:26.590525Z",
     "shell.execute_reply.started": "2024-03-24T15:42:26.582750Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = np.concatenate((new_traning_data, new_valid_dataset), axis=0), np.concatenate((y_train, y_valid), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:42:26.595633Z",
     "iopub.status.busy": "2024-03-24T15:42:26.592881Z",
     "iopub.status.idle": "2024-03-24T15:42:26.612027Z",
     "shell.execute_reply": "2024-03-24T15:42:26.610938Z",
     "shell.execute_reply.started": "2024-03-24T15:42:26.595589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3602, 768)\n",
      "(1120, 768)\n"
     ]
    }
   ],
   "source": [
    "print(new_traning_data.shape)\n",
    "print(new_valid_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:42:26.613854Z",
     "iopub.status.busy": "2024-03-24T15:42:26.613502Z",
     "iopub.status.idle": "2024-03-24T15:45:09.445306Z",
     "shell.execute_reply": "2024-03-24T15:45:09.444260Z",
     "shell.execute_reply.started": "2024-03-24T15:42:26.613826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 10, 768)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 100, 768)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 768)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 10, 768)   0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 10, 10, 768)  0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 1, 768)       0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 1, 10, 768)   0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 10, 768)      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 10, 10, 768)  0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mean_aggregator (MeanAggregator multiple             49216       dropout_20[0][0]                 \n",
      "                                                                 dropout_19[0][0]                 \n",
      "                                                                 dropout_22[0][0]                 \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 10, 64)    0           mean_aggregator[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 1, 64)        0           mean_aggregator[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 1, 10, 64)    0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mean_aggregator_1 (MeanAggregat (None, 1, 32)        2080        dropout_24[0][0]                 \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 32)           0           mean_aggregator_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 32)           0           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            99          lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 51,395\n",
      "Trainable params: 51,395\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "463/463 [==============================] - 5s 8ms/step - loss: 0.9261 - acc: 0.5981 - val_loss: 0.7947 - val_acc: 0.6451\n",
      "Epoch 2/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.8315 - acc: 0.6367 - val_loss: 0.7580 - val_acc: 0.6971\n",
      "Epoch 3/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.7736 - acc: 0.6748 - val_loss: 0.7117 - val_acc: 0.7225\n",
      "Epoch 4/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.7396 - acc: 0.6891 - val_loss: 0.6978 - val_acc: 0.7373\n",
      "Epoch 5/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.7237 - acc: 0.7042 - val_loss: 0.6338 - val_acc: 0.7578\n",
      "Epoch 6/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.7141 - acc: 0.7042 - val_loss: 0.6382 - val_acc: 0.7431\n",
      "Epoch 7/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6972 - acc: 0.7156 - val_loss: 0.6480 - val_acc: 0.7363\n",
      "Epoch 8/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6852 - acc: 0.7153 - val_loss: 0.6128 - val_acc: 0.7745\n",
      "Epoch 9/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6786 - acc: 0.7147 - val_loss: 0.6267 - val_acc: 0.7529\n",
      "Epoch 10/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6803 - acc: 0.7231 - val_loss: 0.6391 - val_acc: 0.7569\n",
      "Epoch 11/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6813 - acc: 0.7131 - val_loss: 0.5910 - val_acc: 0.7657\n",
      "Epoch 12/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6764 - acc: 0.7283 - val_loss: 0.6082 - val_acc: 0.7618\n",
      "Epoch 13/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6693 - acc: 0.7231 - val_loss: 0.5881 - val_acc: 0.7696\n",
      "Epoch 14/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6646 - acc: 0.7161 - val_loss: 0.6126 - val_acc: 0.7686\n",
      "Epoch 15/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6610 - acc: 0.7258 - val_loss: 0.5660 - val_acc: 0.7804\n",
      "Epoch 16/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6627 - acc: 0.7229 - val_loss: 0.6194 - val_acc: 0.7569\n",
      "Epoch 17/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6660 - acc: 0.7202 - val_loss: 0.5837 - val_acc: 0.7804\n",
      "Epoch 18/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6529 - acc: 0.7318 - val_loss: 0.5909 - val_acc: 0.7637\n",
      "Epoch 19/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6674 - acc: 0.7215 - val_loss: 0.6035 - val_acc: 0.7657\n",
      "Epoch 20/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6572 - acc: 0.7320 - val_loss: 0.6035 - val_acc: 0.7657\n",
      "Epoch 21/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6549 - acc: 0.7250 - val_loss: 0.5892 - val_acc: 0.7794\n",
      "Epoch 22/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6490 - acc: 0.7320 - val_loss: 0.6060 - val_acc: 0.7676\n",
      "Epoch 23/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6462 - acc: 0.7299 - val_loss: 0.5942 - val_acc: 0.7765\n",
      "Epoch 24/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6489 - acc: 0.7326 - val_loss: 0.5884 - val_acc: 0.7706\n",
      "Epoch 25/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6453 - acc: 0.7310 - val_loss: 0.5910 - val_acc: 0.7824\n",
      "Epoch 26/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6294 - acc: 0.7396 - val_loss: 0.5763 - val_acc: 0.7833\n",
      "Epoch 27/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6294 - acc: 0.7426 - val_loss: 0.5612 - val_acc: 0.7843\n",
      "Epoch 28/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6427 - acc: 0.7374 - val_loss: 0.6022 - val_acc: 0.7765\n",
      "Epoch 29/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6292 - acc: 0.7445 - val_loss: 0.6329 - val_acc: 0.7461\n",
      "Epoch 30/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6353 - acc: 0.7391 - val_loss: 0.5883 - val_acc: 0.7804\n",
      "Epoch 31/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6403 - acc: 0.7399 - val_loss: 0.5660 - val_acc: 0.7784\n",
      "Epoch 32/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6260 - acc: 0.7434 - val_loss: 0.5730 - val_acc: 0.7863\n",
      "Epoch 33/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6388 - acc: 0.7369 - val_loss: 0.6022 - val_acc: 0.7676\n",
      "Epoch 34/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6261 - acc: 0.7412 - val_loss: 0.5849 - val_acc: 0.7696\n",
      "Epoch 35/50\n",
      "463/463 [==============================] - 3s 6ms/step - loss: 0.6308 - acc: 0.7439 - val_loss: 0.5608 - val_acc: 0.7902\n",
      "Epoch 36/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6341 - acc: 0.7404 - val_loss: 0.5787 - val_acc: 0.7775\n",
      "Epoch 37/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6241 - acc: 0.7458 - val_loss: 0.5787 - val_acc: 0.7804\n",
      "Epoch 38/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6224 - acc: 0.7453 - val_loss: 0.5577 - val_acc: 0.7922\n",
      "Epoch 39/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6241 - acc: 0.7442 - val_loss: 0.5852 - val_acc: 0.7784\n",
      "Epoch 40/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6112 - acc: 0.7528 - val_loss: 0.6031 - val_acc: 0.7686\n",
      "Epoch 41/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6258 - acc: 0.7428 - val_loss: 0.5592 - val_acc: 0.7814\n",
      "Epoch 42/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6207 - acc: 0.7458 - val_loss: 0.5694 - val_acc: 0.7882\n",
      "Epoch 43/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6201 - acc: 0.7469 - val_loss: 0.5945 - val_acc: 0.7696\n",
      "Epoch 44/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6188 - acc: 0.7482 - val_loss: 0.5729 - val_acc: 0.7784\n",
      "Epoch 45/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6155 - acc: 0.7555 - val_loss: 0.5447 - val_acc: 0.7843\n",
      "Epoch 46/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6164 - acc: 0.7501 - val_loss: 0.5571 - val_acc: 0.7814\n",
      "Epoch 47/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6037 - acc: 0.7526 - val_loss: 0.5810 - val_acc: 0.7725\n",
      "Epoch 48/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6275 - acc: 0.7431 - val_loss: 0.5646 - val_acc: 0.7824\n",
      "Epoch 49/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6061 - acc: 0.7539 - val_loss: 0.5635 - val_acc: 0.7941\n",
      "Epoch 50/50\n",
      "463/463 [==============================] - 3s 7ms/step - loss: 0.6107 - acc: 0.7466 - val_loss: 0.5659 - val_acc: 0.7941\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.5659 - acc: 0.7941\n",
      "Test accuracy: 0.7941176295280457\n"
     ]
    }
   ],
   "source": [
    "import stellargraph as sg\n",
    "from stellargraph.layer import GraphSAGE\n",
    "from stellargraph.mapper import GraphSAGENodeGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "\n",
    "# Construct a graph from the DataFrame\n",
    "graph = sg.StellarGraph(nodes=df)\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.index, y, test_size=0.2160, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Define the GraphSAGE model\n",
    "generator = GraphSAGENodeGenerator(graph, batch_size=8, num_samples=[10, 10])\n",
    "train_gen = generator.flow(X_train, y_train)\n",
    "\n",
    "graphsage_model = GraphSAGE(\n",
    "    layer_sizes=[64, 32],\n",
    "    generator=generator,\n",
    "    bias=True,\n",
    "    dropout=0.2,\n",
    "    normalize=\"l2\"\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "x_inp, x_out = graphsage_model.in_out_tensors()\n",
    "prediction = layers.Dense(units=3, activation=\"softmax\")(x_out)\n",
    "\n",
    "model = models.Model(inputs=x_inp, outputs=prediction)\n",
    "learning_rate = 0.001  # Set your desired learning rate here\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Define a checkpoint callback\n",
    "checkpoint_path = \"/kaggle/working/best_model_gnn.h5\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "\n",
    "# Train the model using the generator\n",
    "history = model.fit(train_gen,\n",
    "                    epochs=50,\n",
    "                    validation_data=generator.flow(X_test, y_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpoint])  # Add checkpoint callback\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(generator.flow(X_test, y_test))\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:45:09.447728Z",
     "iopub.status.busy": "2024-03-24T15:45:09.446803Z",
     "iopub.status.idle": "2024-03-24T15:45:10.209323Z",
     "shell.execute_reply": "2024-03-24T15:45:10.208407Z",
     "shell.execute_reply.started": "2024-03-24T15:45:09.447685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Making predictions on the validation dataset\n",
    "y_pred = model.predict(generator.flow(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T15:45:10.211013Z",
     "iopub.status.busy": "2024-03-24T15:45:10.210692Z",
     "iopub.status.idle": "2024-03-24T15:45:10.226337Z",
     "shell.execute_reply": "2024-03-24T15:45:10.224997Z",
     "shell.execute_reply.started": "2024-03-24T15:45:10.210983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7797660943290613\n",
      "Precision: 0.7812320012404456\n",
      "Recall: 0.7941176470588235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# Converting probabilities to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "\n",
    "# Computing F1 score\n",
    "f1 = f1_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Computing precision\n",
    "precision = precision_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Computing recall\n",
    "recall = recall_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "print(\"Recall:\", recall)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4451489,
     "sourceId": 7641359,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30203,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
